{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e4574b-ade4-42c1-8eb9-405890613a0d",
   "metadata": {},
   "source": [
    "# model2regex Prototyping\n",
    "\n",
    "As part of my master thesis with the chair MLSEC at TU Berlin, I am currently working on a machine learning approach to generate a regular expression of Domain Generating Algorithms (DGA) from a Language model (LM). In this prototype we learn an LM with pytorch using the GRU module, we then combine this with a Feed Forward Network (FFN) [[1]] to solve the classification problem of classifying a given domain as generated by the DGA. \n",
    "\n",
    "[1]: https://web.stanford.edu/~jurafsky/slp3/9.pdf#page=9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf2970-b274-4421-953a-6cb8d6f17285",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ae58b9-e22d-466f-a6ce-e28a8bb73c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dga import banjori, simple_dga # import the algorithms from the dga.py file\n",
    "from typing import Callable\n",
    "import logging\n",
    "import sys\n",
    "# Paths\n",
    "real_domains_path = 'data/top-1m.csv' # write the full or relative path to a csv file containing a list of real domains in with rows in the form [#rank, domain]\n",
    "\n",
    "# DGA settings\n",
    "dga_algorithm : Callable[[str], str] = banjori # the algorithm to use can also be any callable that returns a list of domains\n",
    "dga_seed: str = 'earnestnessbiophysicalohax.com' # the initial seed of our dga algorithm\n",
    "\n",
    "# Model settings\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "embed_dim = 64\n",
    "device ='cuda:0' # the GPU device to use\n",
    "\n",
    "#logging\n",
    "level = logging.DEBUG\n",
    "format = '{message}'\n",
    "logging.basicConfig(level=level, format=format, stream=sys.stdout, style='{')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57237d0-aaaa-4781-8259-12de042472c3",
   "metadata": {},
   "source": [
    "## Classifier Network\n",
    "\n",
    "First we are going to define our network module with the help of Pytorch.\n",
    "The module contains an embedding layer, an rnn layer for the language model part and ends in a simple 1 layer network which will output a single class property. The whole forward function in the end will output both the class and the hidden state of the rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96625a9-f119-4384-a193-4dafdea6e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DGAClassifier(nn.Module):\n",
    "    def __init__(self, vocabSize: int, emb: int, size: int, nlayers: int):\n",
    "        super(DGAClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabSize,\n",
    "                                      embedding_dim=emb)\n",
    "        self.rnn = nn.GRU(input_size=emb, hidden_size=size,\n",
    "                          num_layers=nlayers)\n",
    "        self.out = nn.Linear(in_features=size, out_features=1)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        x = hidden_state[-1, :]\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        x = self.sig(x)\n",
    "        return x, hidden_state.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc77c2-c2f0-4dd8-a1fc-de2535cb1ed6",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "To learn the network we've build we are going to generate a [pytorch Dataset][1]. The dataset contains both legitimate Domains from a dataset of the top [1 million domains][2]. \n",
    "The Dataset will contain a few helper functions to make working with it inside the training code simpler.\n",
    "\n",
    "[1]: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "[2]: https://github.com/PeterDaveHello/top-1m-domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54dea1c-2ca8-4de8-b31a-be79d8b3ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from functools import singledispatchmethod\n",
    "\n",
    "class BiMapping:\n",
    "    def __init__(self, characters: str):\n",
    "        characters = set(characters)\n",
    "        self._dict = {ch: i for i,ch in enumerate(characters, start=1)}\n",
    "        self._reverse = {i: ch for i,ch in enumerate(characters, start=1)}\n",
    "        \n",
    "    @singledispatchmethod \n",
    "    def __getitem__(self, item):\n",
    "        raise NotImplementedError(\"item must be of type int or str\")\n",
    "    @__getitem__.register\n",
    "    def _(self, item: str):\n",
    "        return self._dict[item]\n",
    "    @__getitem__.register\n",
    "    def _(self, item: int):\n",
    "        return self._reverse[item]\n",
    "\n",
    "class DomainsAndDGA(Dataset):\n",
    "    def __init__(self, domains: Sequence[Tuple[str, int]]):\n",
    "        self.data = domains\n",
    "        self.max_size = len(max(self.data, key=lambda d: len(d[0]))[0])\n",
    "        self.chars = sorted(list(set(chain(*[d[0] for d in self.data]))))\n",
    "        self.vocabSize = len(self.chars) + 1\n",
    "        self.char_bimap = BiMapping(self.chars)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def isEndChar(self, ind) -> bool:\n",
    "        \"\"\"helper function that checks if  the current index is the 'end' character (index 0)\"\"\"\n",
    "        return ind == 0\n",
    "    def charTensor(self, _input: str) -> torch.Tensor:\n",
    "        \"\"\"Helper function to turn an input string into a tensor of indices\"\"\"\n",
    "        return torch.tensor([[self.char_bimap[c] for c in _input]]).permute(1,0)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        item, label = self.data[idx]\n",
    "        item = torch.tensor([self.char_bimap[c] for c in item])\n",
    "        # we need tensors of same size, so if any domain has a different size we then pad it with 0 which will be our \"end char\"\n",
    "        item = F.pad(item, (0,self.max_size - len(item)), \"constant\", 0)\n",
    "        return (item, torch.tensor(label, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "390bbd8d-64e9-41e8-8223-8eef7d1a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain, repeat\n",
    "from dga import generate_dataset\n",
    "from pathlib import Path\n",
    "#########################\n",
    "# Preparing the Dataset #\n",
    "#########################\n",
    "top1m = pd.read_csv(Path(real_domains_path))\n",
    "real_domains = top1m.values[:,1]\n",
    "real_domains = list(\n",
    "    tuple(\n",
    "        zip(real_domains, repeat(1))\n",
    "    )\n",
    ")\n",
    "dga_domains = generate_dataset(algorithm=dga_algorithm, seed=dga_seed, size=len(real_domains))\n",
    "dga_domains = list(\n",
    "    tuple(\n",
    "        zip(dga_domains, repeat(0))\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = list(chain(real_domains, dga_domains))\n",
    "dataset = DomainsAndDGA(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee60b6f-78b1-40f9-8359-d5f1fc08d2c0",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now that everything is set up, it is time to run the training. We create a class for the Training to easily modify our training settings in a few lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfd8f20-2ad8-4067-8434-c26095a16703",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "\n",
    "class ModelTrain:\n",
    "    def __init__(self, dataset: DomainsAndDGA, model: DGAClassifier, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.untrained_model_path = './untrained_model.pth'\n",
    "        torch.save(model.state_dict(), self.untrained_model_path)\n",
    "        self.criterion = kwargs.get('criterion', nn.CrossEntropyLoss(reduction='mean'))\n",
    "        self.optimizer = kwargs.get('optimizer', optim.Adam(self.model.parameters(), lr=kwargs.get('optim_lr', 0.001)))\n",
    "        self.device = kwargs.get('device', 'cuda:0')\n",
    "        self.save_path = kwargs.get('save_path', './model-fold-{}-path')\n",
    "\n",
    "    def train(self, *, k=5, save_model=True):\n",
    "        kfold = KFold(n_splits=k, shuffle=True)\n",
    "        accuracies = []\n",
    "        for fold, (train_dataset, test_dataset) in enumerate(kfold.split(dataset)):\n",
    "            writer = SummaryWriter(f'./runs/Classifier-prototype-fold-{fold}')\n",
    "            checkpoint = torch.load(self.untrained_model_path)\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "            train_sampler = SubsetRandomSampler(train_dataset)\n",
    "            test_sampler = SubsetRandomSampler(test_dataset)\n",
    "            trainloader = DataLoader(dataset, batch_size=500, sampler=train_sampler)\n",
    "            testloader = DataLoader(dataset, batch_size=500, sampler=test_sampler)\n",
    "            \n",
    "            self.train_fold(dataloader=trainloader, writer=writer)\n",
    "            if save_model:\n",
    "                logging.info(\"saving model to %s\", self.save_path.format(fold))\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                }, self.save_path.format(fold))    \n",
    "            total, correct = self.validate_fold(dataloader=testloader)\n",
    "            accuracy = correct / total\n",
    "            accuracies.append(accuracy)\n",
    "            logging.info(f'Accuracy for fold {fold}: {accuracy:%}')\n",
    "            writer.close()\n",
    "        logging.info(\"Showing summary...\")\n",
    "        for idx, accuracy in enumerate(accuracies):\n",
    "            logging.info(f\"accuracy of fold {idx}: {accuracy:%}\")\n",
    "    def validate_fold(self, *, dataloader: DataLoader):\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for batch, (x,y) in enumerate(dataloader):\n",
    "                output, _ = self.model(x.permute(1,0).to(self.device), None)\n",
    "                total += y.size(0)\n",
    "                correct += (output.permute(1,0).round() == y.to(self.device)).sum().item()\n",
    "        return total , correct\n",
    "\n",
    "        \n",
    "    def train_fold(self, *, dataloader: DataLoader, epochs=10, writer: SummaryWriter):\n",
    "        criterion = self.criterion\n",
    "        device = self.device\n",
    "        model = self.model\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = self.optimizer\n",
    "        h0 = None\n",
    "        for epoch in range(epochs):\n",
    "            logging.info(\"epoch: %s\\n\\n\", epoch)\n",
    "            for batch, (x,y) in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                if h0 is not None:\n",
    "                    h0 = h0.to(device)\n",
    "                output, _ = model(x.permute(1,0).to(device), h0)\n",
    "                if batch % 500 == 0:\n",
    "                    idx = random.randint(0, len(x)-1)\n",
    "                    logging.info(\"----------------------------------------------------\")\n",
    "                    logging.info(f\"showing one prediction for random sample of batch: {batch:,}\")\n",
    "                    logging.info(\"inputstr: %s\", ''.join(dataset.char_bimap[c] for c in x[idx].tolist() if c != 0))\n",
    "                    logging.info(\"real class:\\t%-d\", y[idx].item())\n",
    "                    logging.info(\"output:\\t\\t%-d\", output[idx].round().item())\n",
    "                    logging.info(\"----------------------------------------------------\")\n",
    "                loss = criterion(output.squeeze(), y.to(device))\n",
    "                if writer:\n",
    "                    writer.add_scalar('Loss/train', loss, epoch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch % 500 == 0:\n",
    "                    logging.info(f\"loss at batch {batch:,}: {loss.item()}\")\n",
    "            model.train()\n",
    "        if writer:\n",
    "            writer.flush()\n",
    "        return model, h0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb7e1e-9f62-4ca8-8549-901dab79ecef",
   "metadata": {},
   "source": [
    "Here we are initializing the model and the trainer class and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454753a-b1dc-4b4f-8bc5-1daa861bf6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 0\n",
      "inputstr: xhcnestnessbiophysicalohax.com\n",
      "real class:\t0\n",
      "output:\t\t0\n",
      "----------------------------------------------------\n",
      "loss at batch 0: 1529.505615234375\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 500\n",
      "inputstr: bprxestnessbiophysicalohax.com\n",
      "real class:\t0\n",
      "output:\t\t1\n",
      "----------------------------------------------------\n",
      "loss at batch 500: 1401.2078857421875\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 1,000\n",
      "inputstr: thavestnessbiophysicalohax.com\n",
      "real class:\t0\n",
      "output:\t\t0\n",
      "----------------------------------------------------\n",
      "loss at batch 1,000: 1377.0853271484375\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 1,500\n",
      "inputstr: s1.lunacorgie.xyz\n",
      "real class:\t1\n",
      "output:\t\t1\n",
      "----------------------------------------------------\n",
      "loss at batch 1,500: 1585.1844482421875\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 2,000\n",
      "inputstr: dzckestnessbiophysicalohax.com\n",
      "real class:\t0\n",
      "output:\t\t0\n",
      "----------------------------------------------------\n",
      "loss at batch 2,000: 1547.1162109375\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 2,500\n",
      "inputstr: 1434145105.rsc.cdn77.org\n",
      "real class:\t1\n",
      "output:\t\t1\n",
      "----------------------------------------------------\n",
      "loss at batch 2,500: 1345.8084716796875\n",
      "epoch: 1\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "showing one prediction for random sample of batch: 0\n",
      "inputstr: zrxoestnessbiophysicalohax.com\n",
      "real class:\t0\n",
      "output:\t\t0\n",
      "----------------------------------------------------\n",
      "loss at batch 0: 1439.8057861328125\n"
     ]
    }
   ],
   "source": [
    "model = DGAClassifier(dataset.vocabSize, embed_dim, hidden_size, num_layers)\n",
    "trainer = ModelTrain(dataset, model)\n",
    "\n",
    "trainer.train(k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
